{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DL_자연어 처리 .ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM4wEq8GO0cIC+WAzwfbE22"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"aymF6NtRkarb","colab_type":"text"},"source":["# 딥러닝을 이용한 자연어 처리 \n","* 인공지능 플랫폼이 갖춰야 할 필수 능력 중하나가 사람의 언어를 이해하는 것. \n","* 자연어처리(Natural Language Processing, NLP)는 음성이나 텍스트를 컴퓨터가 인식하고 처리하는것.\n","* 컴퓨터 알고리즘은 수치로 된 데이터만 이해하기 때문에 텍스트를 정제하는 전처리 과정이 필수 "]},{"cell_type":"markdown","metadata":{"id":"5EMk2B1RlnkJ","colab_type":"text"},"source":["##1.텍스트의 토큰화 "]},{"cell_type":"code","metadata":{"id":"z6Dnyg5Plq7K","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1593498300467,"user_tz":-540,"elapsed":1035,"user":{"displayName":"김준연","photoUrl":"","userId":"02482623906254680950"}},"outputId":"6aaf0cc1-6538-4bb1-a38b-7ae08328b8d2"},"source":["from tensorflow.keras.preprocessing.text import text_to_word_sequence\n","text = '해보지 않으면 해낼 수 없다.'\n","result = text_to_word_sequence(text)\n","print('토큰화 :\\n',result)"],"execution_count":55,"outputs":[{"output_type":"stream","text":["토큰화 :\n"," ['해보지', '않으면', '해낼', '수', '없다']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mxgEe-opl5id","colab_type":"text"},"source":["* 주어인 텍스트를 단어 단위로 쉽게 나눠주는 text_to_word_sequence 함수를 사용해준다. "]},{"cell_type":"code","metadata":{"id":"N2U8BWgemDlr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":74},"executionInfo":{"status":"ok","timestamp":1593498300468,"user_tz":-540,"elapsed":1015,"user":{"displayName":"김준연","photoUrl":"","userId":"02482623906254680950"}},"outputId":"fb6641c1-97bc-45eb-ea21-e9b3e77089a4"},"source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","docs = ['먼저 텍스트의 각 단어를 나누어 토큰화합니다.',\n","        '텍스트의 단어로 토큰화해야 딥러닝에서 인식됩니다.',\n","        '토큰화한 결과는 딥러닝에서 사용할 수 있습니다.']\n","token = Tokenizer()\n","token.fit_on_texts(docs)\n","print('단어 갯수 카운트 :\\n', token.word_counts)"],"execution_count":56,"outputs":[{"output_type":"stream","text":["단어 갯수 카운트 :\n"," OrderedDict([('먼저', 1), ('텍스트의', 2), ('각', 1), ('단어를', 1), ('나누어', 1), ('토큰화합니다', 1), ('단어로', 1), ('토큰화해야', 1), ('딥러닝에서', 2), ('인식됩니다', 1), ('토큰화한', 1), ('결과는', 1), ('사용할', 1), ('수', 1), ('있습니다', 1)])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Nv6JWRBMmdss","colab_type":"text"},"source":["* 케라스의 Tokenizer() 함수를 사용하면 단어의 빈도수를 쉽계 계산한다. \n","* word_counts를 이용해 각 문장에서 토큰화된 단어의 빈도수를 리턴해준다. \n","* 또한 OrderdDict 클래스에 담겨져 있는 상태로 리턴이 되기 때문에, 순서를 기억해준다. "]},{"cell_type":"code","metadata":{"id":"9EjprGWKm3sa","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1593498300469,"user_tz":-540,"elapsed":998,"user":{"displayName":"김준연","photoUrl":"","userId":"02482623906254680950"}},"outputId":"4dd3be99-ac8e-42af-d78a-999d0a0479a9"},"source":["print('문장 갯수 카운트  :',token.document_count)"],"execution_count":57,"outputs":[{"output_type":"stream","text":["문장 갯수 카운트  : 3\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mmNkWn3pnGkD","colab_type":"text"},"source":["* document_count() 함수에는 토큰화한 docs의 문장 개수를 리턴해준다. "]},{"cell_type":"code","metadata":{"id":"nhHldlD4nM1a","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":74},"executionInfo":{"status":"ok","timestamp":1593498300470,"user_tz":-540,"elapsed":981,"user":{"displayName":"김준연","photoUrl":"","userId":"02482623906254680950"}},"outputId":"12f35d8b-e416-4e96-95f7-88b47423b431"},"source":["print('각 단어가 몇개의 문장에 포함되었는가 :\\n',token.word_docs)"],"execution_count":58,"outputs":[{"output_type":"stream","text":["각 단어가 몇개의 문장에 포함되었는가 :\n"," defaultdict(<class 'int'>, {'나누어': 1, '텍스트의': 2, '토큰화합니다': 1, '각': 1, '단어를': 1, '먼저': 1, '인식됩니다': 1, '토큰화해야': 1, '딥러닝에서': 2, '단어로': 1, '토큰화한': 1, '사용할': 1, '결과는': 1, '있습니다': 1, '수': 1})\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"U2t9nlpfnR2S","colab_type":"text"},"source":["* 또한 word_docs() 함수를 통해 각 단어들이 몇개의 문장에 나오는가를 세어서 출력한다. (문장별 단어의 빈도수를 보여줌)"]},{"cell_type":"code","metadata":{"id":"MNH1KETwndaq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":74},"executionInfo":{"status":"ok","timestamp":1593498300471,"user_tz":-540,"elapsed":965,"user":{"displayName":"김준연","photoUrl":"","userId":"02482623906254680950"}},"outputId":"597d5645-da7c-4399-9123-0ce0f1a997c3"},"source":["print('각 단어에 매겨진 인덱스 값 :\\n',token.word_index)"],"execution_count":59,"outputs":[{"output_type":"stream","text":["각 단어에 매겨진 인덱스 값 :\n"," {'텍스트의': 1, '딥러닝에서': 2, '먼저': 3, '각': 4, '단어를': 5, '나누어': 6, '토큰화합니다': 7, '단어로': 8, '토큰화해야': 9, '인식됩니다': 10, '토큰화한': 11, '결과는': 12, '사용할': 13, '수': 14, '있습니다': 15}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AGUi-XjknhXD","colab_type":"text"},"source":["* 각 단어에 매겨진 인덱스 값을 출력하려면 word_index()함수를 사용하면 된다. "]},{"cell_type":"markdown","metadata":{"id":"X-z0ieBbnn7a","colab_type":"text"},"source":["## 2.단어의 원-핫 인코딩"]},{"cell_type":"markdown","metadata":{"id":"y9URJNgmoKoy","colab_type":"text"},"source":["* 단어가 문장의 다른요소와 어떤 관계를 가지고 있는지 알아보는 방법이 필요하다.\n","* 이러한 기법중 가장 기본적인 방법이 원-핫 인코딩(one-hot-encoding)\n","* 어떤 문장을 토큰화 하고, 각 토큰이 배열네에서 해당하는 위치를 1로 바꿔서 백터와 하는 방법"]},{"cell_type":"code","metadata":{"id":"FAtqTYW9pdCH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1593498300471,"user_tz":-540,"elapsed":950,"user":{"displayName":"김준연","photoUrl":"","userId":"02482623906254680950"}},"outputId":"911cb492-2bb7-488d-9e8a-9f50b1a794b4"},"source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.text import text_to_word_sequence\n","\n","text = '오랫동안 꿈꾸는 이는 그 꿈을 닮아간다.'\n","result = text_to_word_sequence(text)\n","token = Tokenizer()\n","token.fit_on_texts([text])\n","print(token.word_index)"],"execution_count":60,"outputs":[{"output_type":"stream","text":["{'오랫동안': 1, '꿈꾸는': 2, '이는': 3, '그': 4, '꿈을': 5, '닮아간다': 6}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"H8UK6n6iqL1l","colab_type":"text"},"source":["* 토큰화 함수를 불러와 단어 단위로 토큰화 하고, 각 단어의 인덱스 값을 출력"]},{"cell_type":"code","metadata":{"id":"CEBEL7dhqTer","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1593498300472,"user_tz":-540,"elapsed":939,"user":{"displayName":"김준연","photoUrl":"","userId":"02482623906254680950"}},"outputId":"f19396c7-9afe-45e4-fefe-8e0e42b74f41"},"source":["x = token.texts_to_sequences([text])\n","x"],"execution_count":61,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[1, 2, 3, 4, 5, 6]]"]},"metadata":{"tags":[]},"execution_count":61}]},{"cell_type":"markdown","metadata":{"id":"__BxN9-Kqkfp","colab_type":"text"},"source":["* text_to_sequences() 함수를 이용해 앞서 만들어진 토큰의 인덱스로만 채워진 배열을 만들어준다. \n","* 이제 1~6까지의 정수로 인덱스 되어 있는 것을 0과 1로만 이루어진 배열로 바꿔주는 to_categorical()함수를 사용해 원-핫 인코딩 과정을 진행한다.\n","* 배열의 가장앞에 0이 추가되므로, 단어 수보다 1이 더 많게 인덱스 숫자를 잡아주는 것에 유의한다. "]},{"cell_type":"code","metadata":{"id":"zHZ9nEvUqzZx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":126},"executionInfo":{"status":"ok","timestamp":1593498300472,"user_tz":-540,"elapsed":931,"user":{"displayName":"김준연","photoUrl":"","userId":"02482623906254680950"}},"outputId":"03e5b534-88ad-4616-c06b-939a8acfff56"},"source":["from keras.utils import to_categorical\n","\n","#인덱스 수에 하나를 추가해서 원-핫 인코딩 배열만들기 \n","word_size = len(token.word_index) +1\n","x = to_categorical(x, num_classes = word_size)\n","for i in range(len(x[0])):\n","  print(x[0][i], result[i])"],"execution_count":62,"outputs":[{"output_type":"stream","text":["[0. 1. 0. 0. 0. 0. 0.] 오랫동안\n","[0. 0. 1. 0. 0. 0. 0.] 꿈꾸는\n","[0. 0. 0. 1. 0. 0. 0.] 이는\n","[0. 0. 0. 0. 1. 0. 0.] 그\n","[0. 0. 0. 0. 0. 1. 0.] 꿈을\n","[0. 0. 0. 0. 0. 0. 1.] 닮아간다\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DPBCL9ceri46","colab_type":"text"},"source":["## 3.단어 임베딩 \n","* 원-핫 인코딩을 그대로 들고가기에는 벡터의 길이가  너무 길어지기 때문에, 공간적 낭비를 해결하기위해 단어 임베딩(Word embedding)을 해준다. \n","* 단어 임베딩은 주어진 배열을 정해진 길이로 압축 시켜준다. \n","* 단어간의 유사도를 계산하여 압축시켜주는 원리로, 적절한 크기의 배열로 바꿔주기 최적의 유사도를 계산하는 학습과정을 거친다. \n","* 이때 등장하는것이 바로 오차역전파. 이러한 과정은 Embedding()함수를 사용해 간단히 해낼 수 있다. "]},{"cell_type":"code","metadata":{"id":"2li-9NzWzBGt","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":360},"executionInfo":{"status":"error","timestamp":1593498301120,"user_tz":-540,"elapsed":1570,"user":{"displayName":"김준연","photoUrl":"","userId":"02482623906254680950"}},"outputId":"b2d1da59-a858-46ec-afd8-1869332eb8e5"},"source":["from keras.layers import Embedding\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","\n","model = Sequential()\n","model.add(Embedding(16,4))"],"execution_count":63,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-63-a75611d22bc5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    172\u001b[0m       raise TypeError('The added layer must be '\n\u001b[1;32m    173\u001b[0m                       \u001b[0;34m'an instance of class Layer. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m                       'Found: ' + str(layer))\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_no_legacy_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: The added layer must be an instance of class Layer. Found: <keras.layers.embeddings.Embedding object at 0x7fd4c39d5a90>"]}]},{"cell_type":"markdown","metadata":{"id":"u8-EhR1zz9oH","colab_type":"text"},"source":["* Embedding() 함수는 최소 2개의 매개변수를 필요로 한다. 입력, 출력, 위의 문구 는 16개의 입력단어를 임베딩후, 출력되는 벡터크기를 4로 한다는 것이다. "]},{"cell_type":"markdown","metadata":{"id":"FExmhdtQ0VSG","colab_type":"text"},"source":["## 4.텍스트를 읽고 긍정, 부정 예측하기 \n","* 영화를 보고 남긴 리뷰를 딥러닝 모델로 학습하여, 각 리뷰가 긍적적이면 1, 부정적이면 0이라는 클래스로 예측해보자. "]},{"cell_type":"code","metadata":{"id":"Fi1e3c4u0ZQ2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":55},"executionInfo":{"status":"ok","timestamp":1593498306303,"user_tz":-540,"elapsed":1025,"user":{"displayName":"김준연","photoUrl":"","userId":"02482623906254680950"}},"outputId":"a8b5926f-416b-45e8-c66b-6e12f9c86ecb"},"source":["from numpy import array\n","docs = ['너무 재밌네요','최고에요','참 잘 만든 영화예요','추천하고 싶은 영화입니다.','한번 더 보고 싶네요',\n","        '글쎄요', '별로에요','생각보다 지루하네요', '연기가 어색해요','재미없어요']\n","\n","classes = array([1,1,1,1,1,0,0,0,0,0])\n","token = Tokenizer()\n","token.fit_on_texts(docs)\n","print(token.word_index)"],"execution_count":64,"outputs":[{"output_type":"stream","text":["{'너무': 1, '재밌네요': 2, '최고에요': 3, '참': 4, '잘': 5, '만든': 6, '영화예요': 7, '추천하고': 8, '싶은': 9, '영화입니다': 10, '한번': 11, '더': 12, '보고': 13, '싶네요': 14, '글쎄요': 15, '별로에요': 16, '생각보다': 17, '지루하네요': 18, '연기가': 19, '어색해요': 20, '재미없어요': 21}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"slHJTrj61C95","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1593498306311,"user_tz":-540,"elapsed":1012,"user":{"displayName":"김준연","photoUrl":"","userId":"02482623906254680950"}},"outputId":"a272c905-d2eb-4876-aa93-64b64cdccd29"},"source":["x = token.texts_to_sequences(docs) #토큰에 지정된 인덱스로 새로운 배열 생성\n","print(x)"],"execution_count":65,"outputs":[{"output_type":"stream","text":["[[1, 2], [3], [4, 5, 6, 7], [8, 9, 10], [11, 12, 13, 14], [15], [16], [17, 18], [19, 20], [21]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"y5Z3eDkK1bhY","colab_type":"text"},"source":["* 각 단어가 1부터 20까지의 숫자로 토큰화 되었다는 것을 알 수 있다. 입력된 리뷰 데이터의 토큰 수가 각각 다르다.\n","* 즉, '최고에요'라는 단어는 하나의 단어로 인식되어, 인덱스가 하나만 존재하는 토큰인 반면에, '참 잘 만든 영화에요'는 네개의 단어으로 인식되어 인덱스가 4개인 토큰으로 저장된다. \n","* 딥러닝 모델에 입력을 하기 위해선 학습 데이터의 길이가 동일해야 된다.\n","* 이렇게 길이를 똑같이 맞춰 주는 작업을 padding이라고 한다.  "]},{"cell_type":"markdown","metadata":{"id":"3XQlwr6D4E2L","colab_type":"text"},"source":["### 텍스트 패딩(Padding)"]},{"cell_type":"code","metadata":{"id":"o8dRhZhy1w-2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":199},"executionInfo":{"status":"ok","timestamp":1593498306312,"user_tz":-540,"elapsed":1003,"user":{"displayName":"김준연","photoUrl":"","userId":"02482623906254680950"}},"outputId":"55eae0d9-1573-4207-ecec-5ed64d984a17"},"source":["from tensorflow.keras.preprocessing.sequence import pad_sequences\n","padded_x = pad_sequences(x,4)\n","print(padded_x)"],"execution_count":66,"outputs":[{"output_type":"stream","text":["[[ 0  0  1  2]\n"," [ 0  0  0  3]\n"," [ 4  5  6  7]\n"," [ 0  8  9 10]\n"," [11 12 13 14]\n"," [ 0  0  0 15]\n"," [ 0  0  0 16]\n"," [ 0  0 17 18]\n"," [ 0  0 19 20]\n"," [ 0  0  0 21]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KSBMwmBo2WgR","colab_type":"text"},"source":["* 케라스에서 제공하는 pad_sequences() 함수를 통해, 인덱스별로 구별되어 토큰화 되어 저장된 단어 배열을 넘겨주어 x별의 길이를 4로 맞춰주어, 모든 입력데이터에 대해 길이를 동일화하는 padding작업을 해줄 수 있다. "]},{"cell_type":"markdown","metadata":{"id":"W14WAVmN27EL","colab_type":"text"},"source":["### 입력될 단어들의 긍정, 부정 정도를, 단어임베딩을 포함하여 결과를 출력해보기. \n","* 임베딩 함수에 필요한 세가지 파라미터는 '입력','출력', 단어 수 이다. "]},{"cell_type":"code","metadata":{"id":"jwrkWbGx30ON","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":799},"executionInfo":{"status":"ok","timestamp":1593498427860,"user_tz":-540,"elapsed":1794,"user":{"displayName":"김준연","photoUrl":"","userId":"02482623906254680950"}},"outputId":"d123a861-ea90-4f74-ca29-baa725ac29d7"},"source":["from tensorflow.keras.layers import Dense, Flatten, Embedding\n","from numpy import array\n","import tensorflow as tf\n","word_size = len(token.word_index) +1\n","\n","model = Sequential()\n","model.add(Embedding(word_size, 8, input_length = 4)) #입력 될 단어수는 우리는 padding과정을 거쳐 각 문장당 4개의 입력으로 분할\n","model.add(Flatten()) # Flatten()함수를 사용해 2차원 배열을 1차원으로 바꿔준다.\n","model.add(Dense(1, activation = 'sigmoid'))\n","model.compile(loss = 'binary_crossentropy', optimizer= 'adam', metrics = ['accuracy'])\n","model.fit(padded_x, classes, epochs= 20, batch_size = 10)\n","print('\\n Accuracy: %.4f' %(model.evaluate(padded_x, classes)[1]))"],"execution_count":70,"outputs":[{"output_type":"stream","text":["Epoch 1/20\n","1/1 [==============================] - 0s 4ms/step - loss: 0.6863 - accuracy: 0.8000\n","Epoch 2/20\n","1/1 [==============================] - 0s 2ms/step - loss: 0.6839 - accuracy: 0.9000\n","Epoch 3/20\n","1/1 [==============================] - 0s 1ms/step - loss: 0.6815 - accuracy: 0.9000\n","Epoch 4/20\n","1/1 [==============================] - 0s 1ms/step - loss: 0.6791 - accuracy: 0.9000\n","Epoch 5/20\n","1/1 [==============================] - 0s 2ms/step - loss: 0.6767 - accuracy: 0.9000\n","Epoch 6/20\n","1/1 [==============================] - 0s 1ms/step - loss: 0.6743 - accuracy: 0.9000\n","Epoch 7/20\n","1/1 [==============================] - 0s 1ms/step - loss: 0.6719 - accuracy: 0.9000\n","Epoch 8/20\n","1/1 [==============================] - 0s 1ms/step - loss: 0.6695 - accuracy: 0.9000\n","Epoch 9/20\n","1/1 [==============================] - 0s 2ms/step - loss: 0.6670 - accuracy: 0.9000\n","Epoch 10/20\n","1/1 [==============================] - 0s 1ms/step - loss: 0.6646 - accuracy: 0.9000\n","Epoch 11/20\n","1/1 [==============================] - 0s 1ms/step - loss: 0.6622 - accuracy: 0.9000\n","Epoch 12/20\n","1/1 [==============================] - 0s 1ms/step - loss: 0.6598 - accuracy: 0.9000\n","Epoch 13/20\n","1/1 [==============================] - 0s 1ms/step - loss: 0.6573 - accuracy: 0.9000\n","Epoch 14/20\n","1/1 [==============================] - 0s 2ms/step - loss: 0.6549 - accuracy: 0.9000\n","Epoch 15/20\n","1/1 [==============================] - 0s 1ms/step - loss: 0.6524 - accuracy: 0.9000\n","Epoch 16/20\n","1/1 [==============================] - 0s 1ms/step - loss: 0.6500 - accuracy: 0.9000\n","Epoch 17/20\n","1/1 [==============================] - 0s 1ms/step - loss: 0.6475 - accuracy: 0.9000\n","Epoch 18/20\n","1/1 [==============================] - 0s 1ms/step - loss: 0.6450 - accuracy: 0.9000\n","Epoch 19/20\n","1/1 [==============================] - 0s 1ms/step - loss: 0.6425 - accuracy: 0.9000\n","Epoch 20/20\n","1/1 [==============================] - 0s 3ms/step - loss: 0.6400 - accuracy: 1.0000\n","1/1 [==============================] - 0s 1ms/step - loss: 0.6375 - accuracy: 1.0000\n","\n"," Accuracy: 1.0000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QXw0xz_137Tc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":199},"executionInfo":{"status":"ok","timestamp":1593498651732,"user_tz":-540,"elapsed":856,"user":{"displayName":"김준연","photoUrl":"","userId":"02482623906254680950"}},"outputId":"2198b8be-0c30-487e-c308-ac89cbfec8e9"},"source":["emotion_list = []\n","for i in model.predict(padded_x):\n","  if i >0.5:\n","    emotion_list.append('Positive')\n","  else: emotion_list.append('Nagative')\n","\n","for i in range(len(docs)):\n","  print(docs[i],':',emotion_list[i])"],"execution_count":73,"outputs":[{"output_type":"stream","text":["너무 재밌네요 : Positive\n","최고에요 : Positive\n","참 잘 만든 영화예요 : Positive\n","추천하고 싶은 영화입니다. : Positive\n","한번 더 보고 싶네요 : Positive\n","글쎄요 : Nagative\n","별로에요 : Nagative\n","생각보다 지루하네요 : Nagative\n","연기가 어색해요 : Nagative\n","재미없어요 : Nagative\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FImFStze7TvN","colab_type":"text"},"source":["## 5.이해결론\n","* 텍스트를 딥러닝 모델에 넣기 위해선? 토큰화 과정이 필요!\n","* ex) '난 참 멋있어' 는 토큰화하면 [[1,2,3]] 이 된다. \n","* 여기서 각각의 숫자가, 단어를 토큰화 했을시의 인덱스 넘버! '난 : 1', '참 : 2', '멋있어 :3'\n","* 여기서, 각각의 토큰화했을때의 인덱스 넘버를 원-핫 인코딩을 해주어야 한다. 1 -> [0,1,0,0], 2 ->[0,0,1,0] ~~\n","* 하지만 여기서 '나도 멋있어'의 문장도 함께 딥러닝 모델의 입력으로 넣어주기위해선?\n","* 입력되는 인덱스 배열의 크기가 다르기 때문에! 이를 맞춰주는 padding과정을 거쳐주어야 한다!\n","* 따라서 위의 코딩을 순서화 하자면?\n","* 1. 주어진 문장들을(docs) 토크나이저를 통한 인덱스 배열로 변환.\n","* 2. 인덱스 배열들의 각각 길이를 맞추기 위한 padding작업\n","* 3. padding작업을 마친 후, 딥러닝 모델 설계 시 벡터의 크기를 맞춰줄 embedding layer 넣어주기 \n","* 4.임베딩 레이어에서, 입력은 토크나이저를 통한 각 문장을 구성하는 단어들의 인덱스(최대) +1, 출력은 각 단어를 구성하는 벡터를 맞춰줄 임의의 수, \n","input_length는 패딩 작업을 거칠 때 맞춰준 각 단어의 인덱스 배열의 구성 수\n","* 5. fitting시, train_feature 로는 패딩된 인덱스배열, train_result는 사정에 정의된대로 알아서.."]}]}